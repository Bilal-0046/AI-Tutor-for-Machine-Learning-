[
  {
    "input": "Working:",
    "output": "Semi-supervised learning trains the model using pseudo-labeled training data as opposed to supervised learning. During training, many other models like neural network models and training methods are introduced to increase accuracy.\nStep 1:First, it uses a very small portion of labeled training data to train the model using supervised learning algorithms. Up until the model produces accurate results, training is continued.\nStep 2:Now algorithm will use a portion of unlabeled training data with pseudo labels. In this step, the output can have less accuracy.Step 3:In this step labeled training data and pseudo-labeled training data are linked.\nStep 4:Unlabeled training data and labeled training data share the same input data.\nStep 5:As we did in the previous phase, train the model once more using the new combined input. It will decrease the number of errors and increase the model's accuracy."
  },
  {
    "input": "Advantages:",
    "output": "It is simple to comprehend.\nIt minimizes the utilization of annotated data.\nThis algorithm is reliable."
  },
  {
    "input": "Disadvantages:",
    "output": "The outcomes of iterations are unstable.\nData at the network level is not covered by it.\nIt is not very accurate."
  },
  {
    "input": "Application of Semi-Supervised Learning:",
    "output": "1. Speech recognition:Because labeling audio requires a lot of time and resources, semi-supervised learning can be utilized to overcome these obstacles and deliver superior results.\n2. Web content classification:To classify information on web pages by assigning relevant labels would require a massive staff of human capital due to the billions of websites that exist and offer all kinds of material. To enhance user experience, many forms of semi-supervised learning are employed to annotate and categorize web material.\n3. Text document classification:Making a text document classifier is another case where semi-supervised learning has been effective. The technique works well in this case since it is quite challenging for human annotators to read through several texts that are wordy in order to assign a simple label, such as a kind or genre.\nExample:\nA text document classifier is a typical illustration of a semi-supervised learning application. In this kind of case, it would be almost impossible to obtain a significant quantity of labeled text documents, making semi-supervised learning the ideal choice. Simply said, it would take too much time to have someone read through complete text documents just to categorize them.\nIn these kinds of situations,semi-supervised semi-supervised algorithms help by learning from a tiny labeled text document data set to recognize a huge amount of unlabeled text document data set in the training set."
  },
  {
    "input": "Seq2Seq with RNNs",
    "output": "In the simplest Seq2Seq model RNNs are used in both the encoder and decoder to process sequential data. For a given input sequence(x_1,x_2, ..., x_T), a RNN generates a sequence of outputs(y_1, y_2, ..., y_T)through iterative computation based on the following equation:\nHere\nh_trepresents hidden state at time step t\nx_trepresents input at time step t\nW_{hx}andW_{yh}represents the weight matrices\nh_{t-1}represents hidden state from the previous time step (t-1)\n\\sigmarepresents the sigmoid activation function.\ny_trepresents output at time step t\nLimitations of Vanilla RNNs:\nVanilla RNNs struggle with long-term dependencies due to the vanishing gradient problem.\nTo overcome this, advanced RNN variants like LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Unit) are used in Seq2Seq models. These architectures are better at capturing long-range dependencies."
  },
  {
    "input": "How Does the Seq2Seq Model Work?",
    "output": "A Sequence-to-Sequence (Seq2Seq) model consists of two primary phases: encoding the input sequence and decoding it into an output sequence."
  },
  {
    "input": "1. Encoding the Input Sequence",
    "output": "The encoder processes the input sequence token by token, updating its internal state at each step.\nAfter processing the entire sequence, the encoder produces a context vector i.e a fixed-length representation summarizing the important information from the input."
  },
  {
    "input": "2. Decoding the Output Sequence",
    "output": "The decoder takes the context vector and generates the output sequence one token at a time. For example, in machine translation:\nInput:  \"I am learning\"\nOutput: \"Je suis apprenant\"\nEach token is predicted based on the context vector and previously generated tokens."
  },
  {
    "input": "3. Teacher Forcing",
    "output": "During training, teacher forcing is commonly used. Instead of feeding the decoder’s own previous prediction as the next input, the actual target token from the training data is provided.\nBenefits:\nAccelerates training\nReduces error propagation"
  },
  {
    "input": "Step 1: Import libraries",
    "output": "We will importpytorch."
  },
  {
    "input": "Step 2: Encoder",
    "output": "We will define:\nEach input token is converted to a dense vector (embedding).\nThe GRU processes the sequence one token at a time, updating its hidden state.\nThe final hidden state is returned as the context vector, summarizing the input sequence."
  },
  {
    "input": "Step 3: Decoder",
    "output": "We will define the decoder:\nTakes the current input token and converts it to an embedding.\nGRU uses the previous hidden state (or context vector initially) to compute the new hidden state.\nThe output is passed through a linear layer to get predicted token probabilities."
  },
  {
    "input": "Step 4: Seq2Seq Model with Teacher Forcing",
    "output": "Batch size & vocab size: extracted from input and decoder.\nEncoding: input sequence → encoder → context vector (hidden).\nStart token: initialize decoder with token 0.\nLoop over max_len:\nDecoder predicts next token.\ntop1 → token with max probability.\nAppend top1 to outputs.\nTeacher forcing: sometimes feed true target token instead of prediction.\nReturn predictions: concatenated sequence of token IDs."
  },
  {
    "input": "Step 5: Usage Example with Outputs",
    "output": "Test with example,\nsrc:random input token IDs.\ntrg:random target token IDs (used for teacher forcing).\noutputs:predicted token IDs for each sequence.\n.T:transpose to show batch sequences as rows.\nOutput:"
  },
  {
    "input": "Applications",
    "output": "Machine Translation: Converts text between languages like English to French.\nText Summarization: Produces concise summaries of documents or news articles.\nSpeech Recognition: Transcribes spoken language into text.\nImage Captioning: Generates captions for images by combining visual features with sequence generation.\nTime-Series Prediction: Predicts future sequences based on past temporal data."
  },
  {
    "input": "Advantages",
    "output": "Flexibility: Can handle tasks like machine translation, text summarization and image captioning with variable-length sequences.\nHandling Sequential Data: Ideal for sequential data like natural language, speech and time series.\nContext Awareness: Encoder-decoder architecture captures the context of the input sequence to generate relevant outputs.\nAttention Mechanism: Focuses on key parts of the input sequence, improving performance, especially for long inputs."
  },
  {
    "input": "Disadvantages",
    "output": "Computationally Expensive: Requires significant resources to train and optimize.\nLimited Interpretability: Hard to understand the model's decision-making process.\nOverfitting: Prone to overfitting without proper regularization.\nRare Word Handling: Struggles with rare words not seen during training."
  },
  {
    "input": "Phase I: Scale Space Peak Selection",
    "output": "The concept of Scale Space deals with the application of a continuous range of Gaussian Filters to the target image such that the chosen Gaussian have differing values of the sigma parameter. The plot thus obtained is called theScale Space. Scale Space Peak Selection depends on theSpatial Coincidence Assumption. According to this, if an edge is detected at thesame location in multiple scales(indicated by zero crossings in the scale space)then we classify it as an actual edge.\nIn 2D images, we can detect the Interest Points using the local maxima/minima inScale Space of Laplacian of Gaussian.A potential SIFT interest point is determined for a given sigma value by picking the potential interest point and considering the pixels in the level above (with higher sigma), the same level, and the level below (with lower sigma than current sigma level). If the point is maxima/minima of all these 26 neighboring points, it is a potential SIFT interest point – and it acts as a starting point for interest point detection."
  },
  {
    "input": "Phase II: Key Point Localization",
    "output": "Key point localization involves the refinement of keypoints selected in the previous stage. Low contrast key-points, unstable key points, and keypoints lying on edges are eliminated. This is achieved by calculating theLaplacianof the keypoints found in the previous stage. The extrema values are computed as follows:\n\nIn the above expression, D represents the Difference of Gaussian. To remove the unstable key points, the value ofzis calculated and if the function value at z is below a threshold value then the point is excluded."
  },
  {
    "input": "Phase III: Assigning Orientation to Keypoints",
    "output": "To achieve detection which is invariant with respect to the rotation of the image, orientation needs to be calculated for the key-points. This is done by considering the neighborhood of the keypoint and calculating the magnitude and direction of gradients of the neighborhood. Based on the values obtained, a histogram is constructed with 36 bins to represent 360 degrees of orientation(10 degrees per bin). Thus, if the gradient direction of a certain point is, say, 67.8 degrees, a value, proportional to the gradient magnitude of this point, is added to the bin representing 60-70 degrees. Histogram peaks above 80% are converted into a new keypoint are used to decide the orientation of the original keypoint."
  },
  {
    "input": "Phase IV: Key Point Descriptor",
    "output": "Finally, for each keypoint, a descriptor is created using the keypoints neighborhood. These descriptors are used for matching keypoints across images. A 16x16 neighborhood of the keypoint is used for defining the descriptor of that key-point. This 16x16 neighborhood is divided into sub-block. Each such sub-block is a non-overlapping, contiguous, 4x4 neighborhood. Subsequently, for each sub-block, an 8 bin orientation is created similarly as discussed in Orientation Assignment. These 128 bin values (16 sub-blocks * 8 bins per block) are represented as a vector to generate the keypoint descriptor."
  },
  {
    "input": "Example: SIFT detector in Python",
    "output": "Running the following script in the same directory with a file named \"geeks.jpg\" generates the \"image-with-keypoints.jpg\" which contains the interest points, detected using the SIFT module in OpenCV, marked using circular overlays.\nBelow is the implementation:\nOutput:"
  },
  {
    "input": "How to perform Singular Value Decomposition",
    "output": "To perform Singular Value Decomposition (SVD) for the matrixA = \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix}, let's break it down step by step.\nStep 1: ComputeA A^T\nStep 2: Find the Eigenvalues ofA A^T\nStep 3: Find the Right Singular Vectors (Eigenvectors ofA^T A)\nStep 4: Compute the Left Singular Vectors (Matrix U)\nStep 5: Final SVD Equation\nThis is the Result SVD matrix of matrix A."
  },
  {
    "input": "Applications of Singular Value Decomposition (SVD)",
    "output": "1.Calculation of Pseudo-Inverse (Moore-Penrose Inverse)\nThe pseudo-inverse is a generalization of the matrix inverse, applicable to non-invertible matrices like low-rank matrices. For an invertible matrix, it equals the inverse.\nDenoted as M^+ , it is calculated using the SVDM = U\\Sigma V^T, whereUandVare orthogonal matrices of left and right singular vectors, and\\Sigmais a diagonal matrix of singular values.\nPseudo-inverse formula:M^+ = V\\Sigma^{-1}U^T, where\\Sigma^{-1}inverts non-zero singular values.\n2.Solving a Set of Homogeneous Linear Equations\nForM x = b, ifb = 0, use SVD to choose a column ofVassociated with a zero singular value.\nIfb \\neq 0, solve by multiplying both sides byM^+:x = M^+ b.\n3.Rank, Range, and Null Space\nThe rank, range, and null space of a matrixMcan be derived from its SVD.\nRank: The rank of matrixMis the number of non-zero singular values in\\Sigma.\nRange: The range of matrixMis the span of the left singular vectors in matrix U corresponding to the non-zero singular values.\nNull Space: The null space of matrixMis the span of the right singular vectors in matrixVcorresponding to the zero singular values.\n4.Curve Fitting Problem\nSingular Value Decomposition can be used to minimize theleast square errorin the curve fitting problem. By approximating the solution using the pseudo-inverse, we can find the best-fit curve to a given set of data points.\n5.Applications in Digital Signal Processing (DSP) and Image Processing\nDigital Signal Processing: SVD can be used to analyze signals and filter noise.\nImage Processing: SVD is used for image compression and denoising. It helps in reducing the dimensionality of image data by preserving the most significant singular values and discarding the rest."
  },
  {
    "input": "Implementation of Singular Value Decomposition (SVD)",
    "output": "In this code, we will try to calculate the Singular value decomposition usingNumpyand Scipy.  We will be calculating SVD, and also performing pseudo-inverse. In the end, we can apply SVD for compressing the image\nOutput:\n\nThe output consists of subplots showing the compressed image for different values of r (5, 10, 70, 100, 200), where r represents the number of singular values used in the approximation. As the value of r increases, the compressed image becomes closer to the original grayscale image of the cat, with smaller values of r leading to more blurred and blocky images, and larger values retaining more details."
  },
  {
    "input": "Use of Stepwise Regression?",
    "output": "The primary use of stepwise regression is to build a regression model that is accurate and parsimonious. In other words, it is used to find the smallest number of variables that can explain the data.\nStepwise regression is a popular method for model selection because it can automatically select the most important variables for the model and build a parsimonious model. This can save time and effort for the data scientist or analyst, who does not have to manually select the variables for the model.\nStepwise regression can also improve the model's performance by reducing the number of variables and eliminating any unnecessary or irrelevant variables. This can help to prevent overfitting, which can occur when the model is too complex and does not generalize well to new data.\nOverall, the use of stepwise regression is to build accurate and parsimonious regression models that can handle complex, non-linear relationships in the data. It is a popular and effective method for model selection in many different domains."
  },
  {
    "input": "Stepwise Regression And Other Regression Models?",
    "output": "Stepwise regression is different from other regression methods because it automatically selects the most important variables for the model. Other regression methods, such asordinary least squares(OLS) and least absolute shrinkage and selection operator (LASSO), require the data scientist or analyst to manually select the variables for the model.\nThe advantage of stepwise regression is that it can save time and effort for the data scientist or analyst, and it can also improve the model's performance by reducing the number of variables and eliminating any unnecessary or irrelevant variables. The disadvantage is that it may not always select the best model, and it can be sensitive to the order in which the variables are added or removed.\nOverall, stepwise regression is a useful method for model selection, but it should be used carefully and in combination with other regression methods to ensure that the best model is selected."
  },
  {
    "input": "Difference between stepwise regression and Linear regression",
    "output": "Linear regressionis a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. In other words, it is a method for predicting a response (or dependent variable) based on one or more predictor variables.\nStepwise regression is a method for building a regression model by adding or removing predictors in a step-by-step fashion. The goal of stepwise regression is to identify the subset of predictors that provides the best predictive performance for the response variable. This is done by starting with an empty model and iteratively adding or removing predictors based on the strength of their relationship with the response variable.\nIn summary, linear regression is a method for modeling the relationship between a response and one or more predictor variables, while stepwise regression is a method for building a regression model by iteratively adding or removing predictors."
  },
  {
    "input": "Implemplementation of Stepwise Regression in Python",
    "output": "To perform stepwise regression inPython, you can follow these steps:\nInstall the mlxtend library by running pip install mlxtend in your command prompt or terminal.\nImport the necessary modules from the mlxtend library, including sequential_feature_selector and linear_model.\nDefine the features and target variables in your dataset.\nInitialize the stepwise regression model with the sequential_feature_selector and specify the type of regression to be used (e.g. linear_model.LinearRegression for linear regression).\nFit the stepwise regression model to your dataset using the fit method.\nUse the k_features attribute of the fitted model to see which features were selected by the stepwise regression."
  },
  {
    "input": "Importing Libraries",
    "output": "To implement stepwise regression, you will need to have the following libraries installed:\nPandas: For data manipulation and analysis.\nNumPy: For working with arrays and matrices.\nSklearn: for machine learning algorithms and preprocessing tools\nmlxtend: for feature selection algorithms\nThe first step is to define the array of data and convert it into a dataframe using the NumPy and pandas libraries. Then, the features and target are selected from the dataframe using theilocmethod."
  },
  {
    "input": "Model Development in Stepwise Regression",
    "output": "Next, stepwise regression is performed using theSequentialFeatureSelector()function from the mlxtend library. This function uses a logistic regression model to select the most important features in the dataset, and the number of selected features can be specified using the k_features parameter.\nAfter the stepwise regression is complete, the selected features are checked using the selected_features.k_feature_names_ attribute and a data frame with only the selected features are created. Finally, the data is split into train and test sets using thetrain_test_split()function from the sklearn library, and a logistic regression model is fit using the selected features. The model performance is then evaluated using the accuracy_score() function from the sklearn library.\nOutput:\nThe difference between linear regression and stepwise regression is that stepwise regression is a method for building a regression model by iteratively adding or removing predictors, while linear regression is a method for modeling the relationship between a response and one or more predictor variables.\nIn the stepwise regression examples, the mlxtend library is used to iteratively add or remove predictors based on their relationship with the response variable, while in the linear regression examples, all predictors are used to fit the model."
  },
  {
    "input": "Architecture of StyleGAN",
    "output": "StyleGAN uses the standardGANframework by modifying the generator while the discriminator remains similar to traditional GANs. These changes helps to fine control over image features and improve image quality. Lets see various architectural components:"
  },
  {
    "input": "1. Progressive Growing of Images",
    "output": "It means instead of generating high-resolution images all at once it starts with very low-resolution images (4×4 pixels) and progressively grows them to high resolution (up to 1024×1024 pixels).\nNew layers are gradually added to both the generator and discriminator during training.\nThis approach stabilizes training by allowing the model to first learn coarse structures before adding fine details.\nProgressive growing leads to smoother training and better image quality overall."
  },
  {
    "input": "2. Bi-linear Sampling",
    "output": "It replaces the nearest neighbor sampling used in previous GANs with bi-linear sampling when resizing feature maps.\nBi-linear sampling applies a low-pass filter during both up-sampling and down-sampling which helps in resulting smoother transitions and less pixelation.\nThis helps to reduce artifacts and produces more natural images."
  },
  {
    "input": "3. Mapping Network and Style Network",
    "output": "Inplace of feeding a random latent vectorzinto the generator, it first passes it through an 8-layer fully connected network.\nThis produces an intermediate vectorwwhich controls image features like texture and lighting.\nThe vectorwis transformed using an affine transformation and then fed into an Adaptive Instance Normalization (AdaIN) layer.\nThe input to the AdaIN isy = (y_s, y_b)which is generated by applying (A) to (w). AdaIN operation is defined by the following equation:\nwhere each feature mapxis normalized separately and then scaled and biased using the corresponding scalar components from styley. Thus the dimensional ofyis twice the number of feature maps(x)on that layer. The synthesis network contains 18 convolutional layers 2 for each of the resolutions (4x4 - 1024x1024)."
  },
  {
    "input": "4. Constant Input and Noise Injection",
    "output": "Unlike traditional GANs that input random noise directly into the generator, it uses a learned constant tensor of size 4×4×512 as input.\nThis focuses the model on applying style changes rather than learning basic structure from noise.\nTo add natural-looking random variations like skin pores, wrinkles or freckles, Gaussian noise is added independently to each convolutional layer during synthesis.\nThis noise introduces stochastic detail without affecting overall structure helps in improving realism."
  },
  {
    "input": "5. Mixing Regularization",
    "output": "To encourage diversity and prevent the network from relying too heavily on a single style vector, StyleGAN uses mixing regularization during training:\nTwo different latent vectorsz_1andz_2are sampled and mixed by applying them to different layers in the generator.\nThis forces the model to produce consistent images even when styles change mid-way helps in improving robustness of features."
  },
  {
    "input": "6. Style Control at Different Resolutions",
    "output": "StyleGAN’s synthesis network controls image style at different resolutions each affecting different aspects of the image:\nEach resolution layer also receives its own noise input which affects randomness at that scale for instance, noise at coarse levels affects broad structure while noise at fine levels creates subtle texture details."
  },
  {
    "input": "7. Feature Disentanglement Studies",
    "output": "To understand how well it separates features, two key metrics are used:\nPerceptual Path Length:Measures how smooth the transition between two generated images is when interpolating between their latent vectors. Shorter path length shows smoother changes.\nLinear Separability: Tests whether certain features like gender, age, etc and can be separated using a simple linear classifier in the latent space which shows how well features are disentangled .\nThese studies show that the intermediate latent spacewis more disentangled and easier to separate than the original latent spacezshowing the effectiveness of the mapping network."
  },
  {
    "input": "Results:",
    "output": "StyleGAN achieves state-of-the-art image quality on theCelebA-HQ datasetwhich is a high-resolution face dataset used for benchmarking.\nNVIDIA also introduced theFlickr-Faces-HQ (FFHQ)dataset which offers more diversity in age, ethnicity and backgrounds. It produces highly realistic images on FFHQ as well.\nHere we calculate FID score using 50, 000 randomly chosen images from the training set and take the lowest distance encountered over the course of training."
  },
  {
    "input": "Use cases",
    "output": "StyleGAN’s ability to generate highly realistic images with fine control has many practical applications:\nFace Generation and Enhancement:It is used to create realistic human faces for entertainment, gaming and virtual avatars. It can generate faces that don’t belong to any real person which are useful for video games, movies or virtual meetings.\nFashion Design:Designers use it to blend different style features helps in exploring new clothing looks, colors and patterns. This speeds up creativity and helps to generate innovative design ideas.\nData Augmentation in Machine Learning:In computer vision it generates synthetic images like faces or vehicles to augment datasets. This is valuable when collecting real data is expensive or limited.\nAnimation and Video Games:It’s detailed facial feature generation supports character creation in games. It helps create varied and realistic faces for characters and NPCs helps in enhancing immersion."
  }
]